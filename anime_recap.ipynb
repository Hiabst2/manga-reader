{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8dec17-bddc-409a-8610-c0ab1a57ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from elevenlabs.client import AsyncElevenLabs\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import concurrent.futures\n",
    "\n",
    "from manga_extraction import extract_all_pages_as_images, save_important_pages, split_volume_into_parts, save_all_pages, extract_panels, scale_base64_image\n",
    "from vision_analysis import analyze_images_with_gpt4_vision, detect_important_pages, get_important_panels, VISION_PRICE_PER_TOKEN \n",
    "from prompts import DRAMATIC_PROMPT, BASIC_PROMPT, BASIC_PROMPT_WITH_CONTEXT,  BASIC_INSTRUCTIONS, KEY_PAGE_IDENTIFICATION_INSTRUCTIONS, KEY_PANEL_IDENTIFICATION_PROMPT, KEY_PANEL_IDENTIFICATION_INSTRUCTIONS\n",
    "from citation_processing import extract_text_and_citations, extract_script\n",
    "from movie_director import make_movie\n",
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca43991-2e27-4c95-9a6f-f2974e0b6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_number = 3\n",
    "manga = \"vinland-saga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbe703-0967-473f-b58c-663e05afabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI()\n",
    "# get elevenlabs api key from dotenv\n",
    "narration_client = AsyncElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "\n",
    "print(\"Extracting all pages from the volume...\")\n",
    "volume_scaled_and_unscaled = extract_all_pages_as_images(f\"{manga}/v{volume_number}/v{volume_number}.pdf\")\n",
    "volume = volume_scaled_and_unscaled[\"scaled\"]\n",
    "volume_unscaled = volume_scaled_and_unscaled[\"full\"]\n",
    "print(\"Total pages in volume:\", len(volume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35d189-d3d8-445e-ad4d-dc94060b4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_reference = extract_all_pages_as_images(f\"{manga}/profile-reference.pdf\")[\"scaled\"]\n",
    "chapter_reference = extract_all_pages_as_images(f\"{manga}/chapter-reference.pdf\")[\"scaled\"]\n",
    "\n",
    "profile_pages = []\n",
    "chapter_pages = [] \n",
    "\n",
    "important_page_tokens = 0\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "print(\"Identifying important pages in the volume...\")\n",
    "# Function to wrap the detect_important_pages call\n",
    "def process_batch(start_idx, pages):\n",
    "    response = detect_important_pages(profile_reference, chapter_reference, pages, client,\n",
    "        KEY_PAGE_IDENTIFICATION_INSTRUCTIONS, KEY_PAGE_IDENTIFICATION_INSTRUCTIONS)\n",
    "    return start_idx, response\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize API calls\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in range(0, len(volume), batch_size):\n",
    "        pages = volume[i:i+batch_size]\n",
    "        futures.append(executor.submit(process_batch, i, pages))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        start_idx, response = future.result()\n",
    "        end_index = start_idx + batch_size - 1\n",
    "        print(f\"Processing pages {start_idx} to {min(end_index, len(volume)-1)}\")\n",
    "        \n",
    "        ip = response[\"parsed_response\"]\n",
    "        print(json.dumps(ip, indent=2))\n",
    "        for page in ip:\n",
    "            if page[\"type\"] == \"profile\":\n",
    "                profile_pages.append(page[\"image_index\"] + start_idx)\n",
    "            elif page[\"type\"] == \"chapter\":\n",
    "                chapter_pages.append(page[\"image_index\"] + start_idx)\n",
    "\n",
    "        important_page_tokens += response[\"total_tokens\"]\n",
    "\n",
    "profile_pages.sort()\n",
    "chapter_pages.sort()\n",
    "\n",
    "print(\"Total tokens to extract profiles and chapters:\", important_page_tokens)\n",
    "print(\"\\n__________\\n\")\n",
    "print(\"Profile pages:\", profile_pages)\n",
    "print(\"Chapter pages:\", chapter_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780436d6-8bdc-4b04-a11d-348eb9638225",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(volume)}\")\n",
    "print(\"\\n__________\\n\")\n",
    "print(\"Saving important pages to disk for QA...\")\n",
    "save_important_pages(volume, profile_pages, chapter_pages, manga, volume_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "21d4ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_profiles = [volume[i] for i in profile_pages]    \n",
    "jobs = split_volume_into_parts(volume, volume_unscaled, chapter_pages, 9)\n",
    "parts = jobs[\"parts\"]\n",
    "jobs_unscaled = jobs[\"unscaled_images\"]\n",
    "jobs = jobs[\"scaled_images\"]\n",
    "\n",
    "# Summarize the images in the first job\n",
    "response = analyze_images_with_gpt4_vision(character_profiles, jobs[0], client, BASIC_PROMPT, BASIC_INSTRUCTIONS)\n",
    "recap = response.choices[0].message.content\n",
    "tokens = response.usage.total_tokens\n",
    "movie_script = extract_text_and_citations(response.choices[0].message.content, jobs[0], jobs_unscaled[0])\n",
    "\n",
    "print(\"\\n\\n\\n_____________\\n\\n\\n\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# iterate thrugh the rest of the jobs while adding context from previous ones\n",
    "for i, job in enumerate(jobs):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    response = analyze_images_with_gpt4_vision(character_profiles, job, client, recap + \"\\n-----\\n\" + BASIC_PROMPT_WITH_CONTEXT, BASIC_INSTRUCTIONS)\n",
    "    recap = recap + \"\\n\\n\" + response.choices[0].message.content\n",
    "    tokens += response.usage.total_tokens\n",
    "    print(\"\\n\\n\\n_____________\\n\\n\\n\")\n",
    "    print(response.choices[0].message.content)\n",
    "    movie_script = movie_script + extract_text_and_citations(response.choices[0].message.content, job, jobs_unscaled[i])\n",
    "\n",
    "print(\"\\n\\n\\n_____________\\n\\n\\n\")\n",
    "print(\"\\n\\n\\n_____________\\n\\n\\n\")\n",
    "print(\"\\n\\n\\n_____________\\n\\n\\n\")\n",
    "\n",
    "narration_script = extract_script(movie_script)\n",
    "print(narration_script)\n",
    "print(\"\\n___________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_panels(movie_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86238437",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of segments:\", len(movie_script))\n",
    "for i, segment in enumerate(movie_script):\n",
    "    print(\"segment\", i, \": \", segment[\"text\"])\n",
    "    all_panels_base64 = [panel for sublist in segment[\"panels\"].values() for panel in sublist]\n",
    "    print(len(all_panels_base64))\n",
    "    print(\"number of panels:\", len(all_panels_base64))\n",
    "    print(\"number of images:\", len(segment[\"images\"]))\n",
    "\n",
    "def process_segment(segment_tuple):\n",
    "    i, segment = segment_tuple  # Unpack the tuple\n",
    "    panels = []\n",
    "    for j, page in enumerate(segment[\"images\"]):\n",
    "        if \"panels\" in segment:\n",
    "            if j not in segment[\"panels\"]:\n",
    "                panels.append(page)\n",
    "            else:\n",
    "                for panel in segment[\"panels\"][j]:\n",
    "                    panels.append(panel)\n",
    "        else:\n",
    "            panels.append(page)\n",
    "    \n",
    "    scaled_panels = [scale_base64_image(p) for p in panels]\n",
    "\n",
    "\n",
    "    response = get_important_panels(profile_reference, scaled_panels, client, \n",
    "        segment[\"text\"] + \"\\n________\\n\" + KEY_PANEL_IDENTIFICATION_PROMPT, KEY_PANEL_IDENTIFICATION_INSTRUCTIONS)\n",
    "\n",
    "    important_panels = response[\"parsed_response\"]\n",
    "    # check if important panels is an array\n",
    "    if not isinstance(important_panels, list):\n",
    "        important_panels = []\n",
    "\n",
    "    ip = []\n",
    "    for p in important_panels:\n",
    "        number = p\n",
    "        if isinstance(number, str):\n",
    "            if number.isdigit():\n",
    "                number = int(number)\n",
    "        if not isinstance(number, int):\n",
    "            continue\n",
    "\n",
    "        if number < len(panels):\n",
    "            ip.append(panels[number])\n",
    "        \n",
    "    \n",
    "    return i, ip, response[\"total_tokens\"]\n",
    "\n",
    "# Initialize variables\n",
    "panel_tokens = 0\n",
    "important_panels_info = {}\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create a list of futures\n",
    "    futures = [executor.submit(process_segment, (i, segment)) for i, segment in enumerate(movie_script)]\n",
    "    \n",
    "    # Collect the results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        i, ip, tokens = future.result()\n",
    "        if ip:\n",
    "            print(\"Important panels for segment\", i, \"exist.\")\n",
    "        else: \n",
    "            print(\"No important panels for segment\", i)\n",
    "        movie_script[i][\"important_panels\"] = ip  # Assign the important panels back to the segment\n",
    "        panel_tokens += tokens\n",
    "\n",
    "\n",
    "ELEVENLABS_PRICE_PER_CHARACTER = 0.0003\n",
    "print(\"Tokens for extracting profiles and chapters:\", important_page_tokens, \" | \", \"${:,.4f}\".format(VISION_PRICE_PER_TOKEN * important_page_tokens))\n",
    "print(\"Tokens for summarization:\", tokens,  \" | \", \"${:,.4f}\".format(VISION_PRICE_PER_TOKEN * tokens))\n",
    "print(\"Tokens for extracting important panels:\", panel_tokens, \" | \", \"${:,.4f}\".format(VISION_PRICE_PER_TOKEN * panel_tokens))\n",
    "total_gpt_tokens = important_page_tokens + tokens + panel_tokens\n",
    "print(\"Total GPT tokens:\", total_gpt_tokens,  \" | \", \"${:,.4f}\".format(VISION_PRICE_PER_TOKEN * (total_gpt_tokens)))\n",
    "print(\"Total elevenlabs characters:\", len(narration_script), \" | \", \"${:,.4f}\".format(ELEVENLABS_PRICE_PER_CHARACTER * (len(narration_script))))\n",
    "print(\"GRAND TOTAL COST\",\" | \", \"${:,.4f}\".format(VISION_PRICE_PER_TOKEN * (total_gpt_tokens) + ELEVENLABS_PRICE_PER_CHARACTER * (len(narration_script))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await make_movie(movie_script, manga, volume_number, narration_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcb9d2-bf68-4d18-b475-c52145efdd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebb25c-7473-4f55-b20b-0eff66908581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d4cc7-37a5-44d3-9480-b98dae3c8289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7aac2-9860-4eb8-8d54-8a4c34702fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33283437-1e28-43a8-b067-b7b13c412bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6edb8-6850-4418-87aa-b9496e4d140d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
